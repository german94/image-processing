\section{Introducción Teórica}
%Contendrá una breve explicación de la base teórica que fundamenta los métodos involu- crados en el
%trabajo, junto con los métodos mismos. No deben incluirse demostraciones de propiedades ni
%teoremas, ejemplos innecesarios, ni definiciones elementales (como por ejemplo la de matriz
%simétrica). En vez de definiciones básicas es conveniente citar ejemplos de bibliografía adecuada.
%Una cita vale más que mil palabras.
%
\subsection{k Nearest Neighbor y Principal Component Analysis}
En el presente trabajo utilizaremos los métodos de kNN y PCA para llevar a cabo el reconocimiento de dígitos manuscritos. Comenzaremos explicando resumidamente los métodos utilizados, de una manera más general. Dado un vector $v$ y un conjunto de vectores $U$, queremos saber a que clase pertenece $v$ (qué dígito es) . Nosotros sabemos a que clase pertenece cada vector $u_i \in U$, entonces una manera sencilla de "estimar" a que clase pertenece $v$ en base a la información que poseemos, es comparándolo con cada vector de $U$. Una manera de efectuar esta comparación es tomando la distancia (norma 2) entre $v$ y cada vector $u_i \in U$ y quedarnos con la clase del $u_m$ que minimice esa distancia sobre todos los demás. Lo que hace el algoritmo kNN es generalizar un poco esta idea. En lugar de quedarnos con la clase del $u_m$ que minimice la distancia a $v$, kNN toma las clases de los $k$ vectores $u_1, u_2, ..., u_k$ cuyas distancias sean mínimas y elige entre ellas la clase que más aparezca. Para conocer el parámetro $k$ es necesario realizar experimentos probando distintos valores y tomar una decisión en base a la calidad de los resultados.
\par El algoritmo kNN es conceptualmente fácil de entender e implementar, lo cual constituye una ventaja, pero su desventaja principal es el tiempo de cómputo que requiere. Esto se debe a que, como los vectores representan imágenes, la dimensión de estos vectores suele ser grande. Dado que la idea detras del reconocimiento de imágenes en este trabajo radica en utilizar una base de datos relativamente grande (de ahora en más dbTrain) para determinar que tipo de imagen es la que estamos tratando de reconocer, el costo de computar estas distancias se multiplica por la cantidad de imágenes que tenemos en nuestra base de datos. Vemos que este método, así como está planteado, no escala.
\par Como ya dijimos, por cuestiones de performance no es conveniente utilizar kNN de manera directa con los datos que tenemos. Lo que vamos a intentar hacer entonces, es realizar una transformación de nuestros datos de manera tal que luego, cuando queramos aplicar kNN no nos resulte tan costoso, este método es conocido como PCA. Recordemos que nuestros datos son vectores en un espacio ${\rm I\!R}^n$, entonces lo que vamos a querer hacer es transformarlos a vectores en otro espacio ${\rm I\!R}^\alpha$ tal que $\alpha < n$. Pero también vamos a querer que esta transformación no "cambie por completo" a nuestros vectores, en el sentido de que sigan representando las imágenes que representaban o al menos conserven las "partes relevantes" de ellas. Para poder llevar a cabo esta transformación realizaremos los siguientes pasos:

\begin{enumerate}
\item Tomamos los vectores de dbTrain que usaremos para comparar la imagen que queremos reconocer en forma matricial (cada vector una fila de la matriz) y hallar la matriz de covarianza $M \in {\rm I\!R}^{nxn}$.
\item Hallar una matriz $P \in {\rm I\!R}^{nxn}$ que nos permita disminuir la covarianza de los datos de dbTrain. Esto equivale a buscar las variables que tengan la mayor varianza entre sí y covarianza 0. El objetivo de esto es disminuir la redundancia en nuestros datos.
\item Sea $P'$ la matriz $P$ con las primeras $\alpha$ columnas (este parámetro se fija mediante experimentación), es decir $P' \in {\rm I\!R}^{nx\alpha}$, y $x_i$ la i-ésima imagen de dbTrain, realizamos el producto $x'_i = P'^t \widehat{x_i}$, ahora $x'_i$ es nuestra nueva i-ésima imagen de train y está en el espacio ${\rm I\!R}^{nx\alpha}$.
\item Sea $x$ una imagen vectorizada cuya clase queremos reconocer, realizamos el producto $x'= P'^t\widehat{x}$ donde $\widehat{x} = (x - \mu) / (\sqrt{n - 1})$, $\mu$ es la media de las imágenes de dbTrain y $n$ la cantidad de imágenes de dbTrain. Como ahora $\widehat{x} \in {\rm I\!R}^{\alpha}$ y los vectores de dbTrain están en el mismo espacio, podemos aplicar kNN con $x'$ y los $x'_i$.
\end{enumerate}

Para obtener $P$ lo que hacemos es hallar la base ortonormal de autovectores de $M$. Sabemos que dicha base existe por ser $M$ simétrica. Entonces la columna $i$ de $P$ va a ser el vector $v_i$, el i-ésimo autovector de $M$. Esta base lo que nos permite hacer es "observar" a nuestros datos desde otro lugar, o sea ahora nuestros ejes de coordenadas van a estar en las direcciones donde más varianza existe entre los datos.

Una vez completados estos tres pasos, cuando queramos reconocer a que clase pertenece un vector $v$, trabajamos con $v'= P'^tv$ y, dado que ahora $v'$ es un vector de $\in {\rm I\!R}^{\alpha}$ al igual que los vectores de dbTrain podemos aplicar $kNN$.

\subsection{Método de la potencia y deflación}
En la sección anterior explicamos los métodos que nos permiten reducir la dimensión de las imágenes con las cuales vamos a trabajar y, en base a cierta informacion que tenemos, efectuar comparaciones para predecir a que clase pertenece una imagen. Vimos que, uno de los pasos de PCA es obtener una matriz $P$ cuyas columnas son los autovectores de otra matriz $M$. El método de la potencia, junto con el método de deflación, nos permite encontrar los autovectores y autovalores asociados a la matriz $M$.
\par Dada una matriz $A \in {\rm I\!R}^{nxn}$ cuyos autovalores $\lambda_1, ..., \lambda_n$ cumplen $|\lambda_1| > |\lambda_2| \geq |\lambda_3| \geq ... \geq |\lambda_n|$ y $v_1, ..., v_n$ los autovectores asociados, el Método de la potencia estimará $v_1$ y $\lambda_1$. Decimos estimará porque para tener el valor exacto deberíamos calcular un límite, entonces vamos a "simular" este límite computacionalmente con lo cual el resultado puede tener cierto error. Este método necesita de un vector inicial $x_0$ y un valor de $k$ relativamente grande y lo que va a hacer es calcular $x_{i + 1} = \frac{Ax_i}{||Ax_i||_2}$ y $\widehat{\lambda}_1 = \frac{\Phi(Ax_{i + 1})}{\Phi(Ax_i)}$ desde $i = 1$ hasta $k$. El resultado será $x_{k + 1} = \widehat{v_1} \approx v_1$ y $\widehat{\lambda}_1 \approx \lambda1$. Para que esto se cumpla es importante aclarar que la función $\Phi: {\rm I\!R}^{n}\rightarrow {\rm I\!R}$ que usamos debe cumplir las siguientes propiedades:
\begin{itemize}
\item Debe ser continua
\item Si $v \in {\rm I\!R}^{n}$ y $v \neq 0$ entonces $\Phi(v) \neq 0$
\item Si $v \in {\rm I\!R}^{n}$ y $\alpha \in {\rm I\!R}$ entonces $\Phi(\alpha v) = \alpha\Phi(v)$
\end{itemize}
El Método de la potencia también pide que el vector inicial $x_0$ no sea perpendicular a $v_1$, sin embargo a la hora de implementarlo en una computadora esto puede no ser tenido en cuenta y no traerá grandes consecuencias. Esto se debe a que nuestro resultado va a estar arrastrando cierto error a medida que el método itere y este error va a estar cambiando la dirección de $x_i$ (aunque sea mínimamente) y en ese caso dejaría de ser perpendicular a $v_1$.
\par Vimos como funciona el Método de la potencia y que nos devuelve el autovalor de mayor módulo junto con su autovector asociado, sin embargo nosotros queremos obtener todos los autovalores y autovectores de $A$. Esto se soluciona definiendo una nueva matriz $\widehat{A}$ y aplicando nuevamente el Método de la potencia pero ahora sobre $\widehat{A}$. Supongamos que ya tenemos los valores $\widehat{v_1} \approx v_1$ y $\widehat{\lambda}_1 \approx \lambda_1$ obtenidos al aplicar el Método de la potencia sobre $A$, entonces para obtener $\widehat{v_2}$ y $\widehat{\lambda}_2$ aplicamos nuevamente el método pero ahora sobre $\widehat{A} = A - \widehat{\lambda}_1\widehat{v_1}\widehat{v_1}^t$. Para el caso general, si queremos obtener $\widehat{v_{i + 1}}$ y $\widehat{\lambda}_{i + 1}$, tenemos $\widehat{v_i}$ y $\widehat{\lambda}_i$ y nuestra matriz es $\widehat{A}$, definimos $\widehat{\widehat{A}} = \widehat{A} - \widehat{\lambda}_i\widehat{v_i}\widehat{v_i}^t$ y aplicamos el método sobre $\widehat{\widehat{A}}$. A esto se le llama \textit{deflación}. Los autovalores y autovectores de la nueva matriz que definimos van a ser los mismos que la matriz original (la primera o la que definimos en el paso anterior) excepto por el autovalor de mayor módulo y su autovector asociado.
\par Si bien vimos que para aplicar el método de la potencia en una matriz $A$ se tiene que cumplir que $A$ tenga un autovalor mayor estricto (de multiplicidad 1) en módulo a todos los demás, para poder hacer deflación iterativamente y obtener todos los autovalores y autovectores de $A$ es necesario que valgan las desigualdades de manera estricta: $|\lambda_1| > |\lambda_2| > |\lambda_3| > ... > |\lambda_n|$. Esto debe ser así para que cada nueva matriz que definimos cumpla las hipótesis que requiere el Método de la potencia.