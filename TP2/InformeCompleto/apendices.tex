\section{Apéndices}

%\usepackage[ruled,vlined]{algorithm2e}

\parindent = 0 pt
\parskip = 5 pt

\addtolength{\topmargin}{-1cm}
\addtolength{\textheight}{1cm}

\newcommand{\real}{\mathbb{R}}
\newcommand{\kknn}{k}
\newcommand{\kpca}{\alpha}
\newcommand{\kkfold}{K}

%En el apéndice A se incluirá el enunciado del TP. En el apéndice B se incluirán los
%códigos fuente de las funciones relevantes desde el punto de vista numérico. Resultados
%que valga la pena mencionar en el trabajo pero que sean demasiado específicos para
%aparecer en el cuerpo principal del trabajo podrán mencionarse en sucesivos apéndices
%rotulados con las letras mayusculas del alfabeto romano. Por ejemplo: la demostración
%de una propiedad que aplican para optimizar el algoritmo que programaron para resolver
%un problema.

\subsection*{Apéndice A:}

\begin{center}
\begin{tabular}{r|cr}
 \begin{tabular}{c}
{\large\bf\textsf{\ M\'etodos Num\'ericos\ }}\\ 
Primer Cuatrimestre 2015\\
{\bf Trabajo Pr\'actico 2}\\
\end{tabular} &
\begin{tabular}{@{} p{1.6cm} @{}}
\includegraphics[width=1.6cm]{logodpt.jpg}
\end{tabular} &
\begin{tabular}{l @{}}
 \emph{Departamento de Computaci\'on} \\
 \emph{Facultad de Ciencias Exactas y Naturales} \\
 \emph{Universidad de Buenos Aires} \\
\end{tabular} 
\end{tabular}
\vskip 10pt
\textbf{\Large }
\end{center}

\vskip 10pt
\hrule
\vskip 5pt

{\bf\noindent Introducci\'on}

Uno de los momentos de mayor tensi\'on durante un cuatrimestre se genera en \'epocas de parciales. El d\'ia del examen, luego de entregar la resoluci\'on del mismo, suele aparecer esa inmanejable incertidumbre en los alumnos: \emph{?`entregu\'e todas las hojas?}, \emph{?`consider\'e el caso particular de $\beta = 0$?}, \emph{?`por qu\'e mi compa\~nero puso verdadero, si yo encontr\'e un contraejemplo?} Desde el lado del docente, la dificultad radica en corregir un gran volumen de ejercicios t\'ecnicamente complejos en un tiempo acotado, buscando deducir en base a informaci\'on limitada si el alumno comprendi\'o o no el tema. 

Estos dos factores llevaron a la conformaci\'on de una comisi\'on interclaustro de docentes y alumnos, para quienes resguardamos su identidad, a proponer como soluci\'on el desarrollo de una herramienta autom\'atica de correcci\'on de ex\'amenes, con el fin de reducir los tiempos en la devoluci\'on de los mismos. Para evitar suspicacias respecto de las correcciones, se propuso dividir el problema en varias etapas y que el sistema sea desarrollado por alumnos, contando desde ya con la validaci\'on de los docentes. La primera etapa del proyecto consiste en desarrollar una herramienta que permita autom\'aticamente identificar d\'igitos manuscritos, a ser aplicada a una digitalizaci\'on de cada examen, y cuyo post-procesamiento involucrar\'a darle sem\'antica y reglas a la informaci\'on obtenida.

Como es de esperar, el \'exito de la herramienta depende inevitablemente de disponer de un buen reconocedor de d\'igitos manuscritos. Para semejante responsabilidad, dado el gran nivel mostrado durante el comienzo del cuatrimestre, la comisi\'on asesora ha decidido elegir a los alumnos de M\'etodos Num\'ericos para llevar adelante el desarrollo.

{\bf\noindent Objetivos y Metodolog\'ia}

Como instancias de entrenamiento, se tiene una base de datos de $n$ im\'agenes de $M \times M$ p\'ixeles, las cuales se encuentran etiquetadas con el d\'igito, 0-9, al que corresponden. Definimos como $m = M \times M$ al n\'umero total de p\'ixeles de una imagen. Asumiremos tambi\'en que las im\'agenes se encuentran en escala de grises (cada pixel corresponde a un valor entre 0 y 255, indicando la intensidad del mismo) y que el etiquetado no contiene errores. El objetivo del trabajo consiste en utilizar la informaci\'on de la base de datos para, dada una nueva imagen de un d\'igito sin etiquetar, determinar a cu\'al corresponde teniendo en cuenta factores de calidad y tiempo de ejecuci\'on requeridos.

Una primera aproximaci\'on es utilizar el conocido algoritmo de \emph{$\kknn$ Vecinos m\'as Cercanos} ($kNN$, por su nombre en ingl\'es). En su versi\'on m\'as simple, este algoritmo considera a cada objeto de la base de entrenamiento como un punto en el espacio, para el cual se conoce a qu\'e clase corresponde (en nuestro caso, qu\'e d\'igito es). Luego, al obtener un nuevo objeto que se busca clasificar simplemente se buscan los $\kknn$ vecinos m\'as cercanos y se le asigna la clase que posea el mayor n\'umero de repeticiones dentro de ese subconjunto, es decir, la moda. Con este objetivo, podemos representar a cada imagen de nuestra base de datos como un vector $x_i \in \real^{m}$, con $i = 1,\dots,n$, y de forma an\'aloga interpretar las im\'agenes a clasificar mediante el algoritmo $kNN$.

Sin embargo, el algoritmo del vecino m\'as cercano es sensible a la dimensi\'on de los objetos a considerar y, a\'un aplicando t\'ecnicas de preprocesamiento, puede resultar muy costoso de computar. Teniendo en cuenta esto, una alternativa interesante de preprocesamiento que busca reducir el tama\~no de los elementos a considerar es el m\'etodo de \emph{An\'alisis de Componentes Principales} (PCA, por su sigla en ingl\'es), que consiste en lo siguiente. Para $i = 1,\ldots, n$, recordar que $x_i \in \real^{m}$ la $i$-\'esima imagen de nuestra base de datos almacenada por filas en un vector, y sea $\mu = (x_1 + \ldots + x_n)/n$ el promedio de las im\'agenes. Definimos $X\in\real^{n\times m}$ como la matriz que contiene en la $i$-\'esima fila al vector $(x_i - \mu)^{t}/\sqrt{n-1}$, y $$A=U \Sigma V^t$$ a su descomposici\'on en valores singulares, con $U\in\real^{n\times n}$ y $V\in\real^{m\times m}$ matrices ortogonales, y $\Sigma\in\real^{n\times m}$ la matriz diagonal conteniendo en la posici\'on $(i,i)$ al $i$-\'esimo valor singular $\sigma_i$. Siendo $v_i$ la columna $i$ de $V$, definimos para $i = 1,\ldots,n$ la \textsl{transformaci\'on caracter\'istica} del d\'igito $x_{i}$ como el vector $\mathbf{tc}(x_i) = (v_1^t\, x_i, v_2^t\, x_i,\ldots,v_{\kpca}^t\, x_i) \in\real^{\kpca}$, donde $\kpca \in\{1,\ldots,m\}$ es un par\'ametro de la implementaci\'on. Este proceso corresponde a extraer las $\kpca$ primeras \textit{componentes principales} de cada imagen. La intenci\'on es que $\mathbf{tc}(x_i)$ resuma la informaci\'on m\'as relevante de la imagen, descartando los detalles o las zonas que no aportan rasgos distintivos. Dada una nueva imagen $x$ de un d\'igito, se calcula $\mathbf{tc}(x)$ y se compara con $\mathbf{tc}(x_i)$, para $i = 1,\ldots, n$, utilizando alg\'un criterio de clasificaci\'on adecuado, como por ejemplo $kNN$.

Finalmente, nos concentramos en la evaluaci\'on de los m\'etodos. Dado que necesitamos conocer a qu\'e d\'igito corresponde una imagen para poder verificar la correctitud de la clasificaci\'on, una alternativa es particionar la base de entrenamiento en dos, utilizando una parte de ella en forma completa para el entrenamiento y la restante como test, pudiendo as\'i corroborar la predicci\'on realizada. Sin embargo, realizar toda la experimentaci\'on sobre una \'unica partici\'on de la base podr\'ia resultar en una incorrecta estimaci\'on de par\'ametros, como por ejemplo el conocido \emph{overfitting}. Luego, se considera la t\'ecnica de \emph{cross validation}, en particular el \emph{$\kkfold$-fold cross validation}, para realizar una estimaci\'on de los par\'ametros del modelo que resulte estad\'isticamente m\'as robusta.\\ 

{\bf\noindent Enunciado}

Se pide implementar un programa en \verb+C+ o \verb-C++- que lea desde archivos las im\'agenes de entrenamiento correspondientes a distintos d\'igitos y que, utilizando los m\'etodos descriptos en la secci\'on anterior, dada una nueva imagen de un d\'igito determine a qu\'e n\'umero pertenece. Para ello, el programa deber\'a implementar el algoritmo de $kNN$ as\'i como tambi\'en la reducci\'on de dimensi\'on previa utilizando PCA.  

Con el objetivo de obtener la descomposici\'on en valores singulares, se deber\'a implementar el m\'etodo de la potencia con deflaci\'on para la estimaci\'on de autovalores/autovectores de la matriz de covarianza. En este contexto, la factibilidad de aplicar este m\'etodo es particularmente sensible al tama\~no de las im\'agenes de la base de datos. Por ejemplo, considerar im\'agenes en escala de grises de $100 \times 100$ p\'ixeles implicar\'ia trabajar con matrices de tama\~no $10000 \times 10000$. Se pide tener en cuenta este factor durante el desarrollo y analizar c\'omo afecta (si es que lo hace) en el desarrollo del trabajo. 

Consideramos la base de datos MNIST, en la versi\'on disponible en \emph{Kaggle}. Esta base contiene un conjunto de datos de entrenamiento de 42.000 d\'igitos manuscritos en escala de grises y con $M = 28$. A su vez, provee un conjunto de datos de test de 28.000 d\'igitos, de similares caracter\'isticas, pero sin el correspondiente etiquetado. En base a estos datos, se pide separar el procedimiento en dos etapas. 

La primera de ellas consiste en realizar un estudio experimental con los m\'etodos propuestos sobre la base de entrenamiento y utilizando la t\'ecnica de \emph{$\kkfold$-fold cross validation} mencionada anteriormente. Para esta \'ultima tarea en particular, se recomienda leer y utilizar la rutina \verb+cvpartition+ provista por MATLAB. Llamamos $\kknn$ a la cantidad de vecinos a considerar en el algoritmo $kNN$, $\kpca$ a la cantidad de componentes principales a tomar y $\kkfold$ a la cantidad de particiones consideradas para el cross-validation. La calidad de los resultados obtenidos ser\'an analizados considerando la tasa de efectivdad lograda, es decir, la cantidad d\'igitos correctamente clasificados respecto a la cantidad total de casos analizados. En funci\'on de la experimentaci\'on se piden como m\'inimo los siguientes experimentos:
\begin{itemize}
\item Analizar el comportamiento y la factibilidad de aplicar el algoritmo $kNN$ para un rango razonable de valores distintos de $\kknn$, analizando el compromiso entre el tiempo de ejecuci\'on y la calidad de los resultados obtenidos. Es posible, como experimento opcional, considerar alguna t\'ecnica alternativa de preprocesamiento de la informaci\'on en caso de ser necesario (y, desde ya, que no sea PCA). 
\item Analizar la calidad de los resultados obtenidos al combinar PCA con $kNN$, para un rango amplio de combinaciones de valores de $\kknn$ y $\kpca$. Considerar en el an\'alisis tambi\'en el tiempo de ejecuci\'on.
\item Realizar los experimentos de los items anteriores para al menos dos valores distintos de $\kkfold$. Justificar el por qu\'e de la elecci\'on de los mismos.
\item En base a los resultados obtenidos para ambos m\'etodos, seleccionar aquella combinaci\'on de par\'ametros que se considere la mejor alternativa, con su correspondiente justificaci\'on, y compararlas entre s\'i y sugerir un m\'etodo para su utilizaci\'on en la pr\'actica.
\end{itemize}

Finalmente, y con los par\'ametros seleccionados en la fase experimental, ejecutar el m\'etodo seleccionado sobre el conjunto de datos de test, utilizando como entrenamiento los 42.000 d\'igitos comprendidos en el conjunto de entrenamiento. Analizar el tiempo de c\'omputo requerido. Dado que la c\'atedra no posee la informaci\'on sobre a qu\'e d\'igito corresponde cada imagen (y la idea no es graficar uno por uno y obtenerlo a mano), se sugiere a cada grupo participar en la competencia \emph{Digit Recognizer} actualmente activa en \emph{Kaggle} realizando el/los env\'ios que consideren apropiados y reportar en el informe los resultados obtenidos.\\

Puntos opcionales (no obligatorios)

\begin{itemize}
\item Mostrar que si tenemos la descomposici\'on $A = U\Sigma V^t$, $V$ es la misma matriz que obtenemos al diagonlizar la matriz de covarianzas.
\item Realizar experimentos utilizando d\'igitos manuscritos creados por el grupo, manteniendo el formato propuesto.\footnote{Notar que en la base original las figuras est\'an rotadas y es blanco sobre negro, y no al rev\'es.} Reportar resultados y dificultades encontradas.
\item Implmenentar alguna mejora al algoritmo de $kNN$.
\end{itemize}

{\bf\noindent Programa y formato de archivos}

Se deber\'an entregar los archivos fuentes que contengan la resoluci\'on del trabajo pr\'actico. El ejecutable tomar\'a tres par\'ametros por l\'inea de comando, que ser\'an el nombre del archivo de entrada, el nombre del archivo de salida, y el m\'etodo a ejectutar (0: $kNN$, 1: PCA + $kNN$).
	

Asumimos que la base de datos de im\'agenes de entrenamiento se llama \verb+train.csv+ y que la base de test \verb+test.csv+, y que ambas siguen el formato establecido por la competencia. Para facilitar la experimentaci\'on, el archivo de entrada con la descripci\'on del experimento tendr\'a la siguiente estructura: 

\begin{itemize}
\item La primera l\'inea contendr\'a el path a la(s) base(s) de datos, $\kknn$, $\kpca$ y $\kkfold$.
\item Luego, habra $\kkfold$ l\'ineas de 42.000 valores, uno por cada muestra de la base de entrenamiento, donde un 1 indicar\'a que esa muestra se considera parte del entrenamiento, y un 0 que se considera parte del test. Luego, de esta forma se pueden codificar las particiones realizadas por el \emph{$\kkfold$-fold cross validation}.
\end{itemize}

El archivo de salida obligatorio tendr\'a para cada partici\'on que figure en el archivo de entrada el vector soluci\'on con los $\kpca$ valores singulares de mayor magnitud, con una componente del mismo por l\'inea. Adem\'as, el programa deber\'a generar un archivo, cuyo formato queda a criterio del grupo, indicando la tasa de reconocimiento obtenida para cada partici\'on. Adicionalmente, se generar\'a un archivo que concatene la extension \verb+.csv+ al segundo valor recibido como par\'ametro del programa, que escribir\'a la predicci\'on realizada sobre la base de test en el formato requerido por la competencia siguiendo el formato establecido por las reglas de la competencia.

Junto con el presente enunciado, se adjunta una serie de scripts hechos en \verb+python+ y un conjunto instancias de test que deber\'an ser utilizados para la compilaci\'on y un testeo b\'asico de la implementaci\'on. Se recomienda leer el archivo \verb+README.txt+ con el detalle sobre su utilizaci\'on.

\vskip 0.5 cm
\hrule
\vskip 0.1 cm

{\bf Sobre la entrega}
\begin{itemize}
\item \textsc{Formato electr\'onico:} Jueves 14 de Mayo de 2015, \underline{hasta las 23:59 hs.}, enviando el trabajo
(\texttt{informe} + \texttt{c\'odigo}) a \texttt{metnum.lab@gmail.com}. El \texttt{asunto} del email debe comenzar con el texto \verb|[TP2]| seguido
de la lista de apellidos de los integrantes del grupo. Ejemplo: \texttt{[TP2] Acevedo, Miranda, Montero}
\item \textsc{Formato f\'isico:} Viernes 15 de Mayo de 2015, de 17:30 a 18:00 hs.
\end{itemize}

\newpage

\subsection{Apéndice B:}

\begin{verbatim}

double ObtenerAutovalorMayor(Matriz<double> &A, Matriz<double>& v0){

    double lambda= normaInf(v0);
    double normalizar= norma2(v0);

    for(int i = 0; i < v0.filas(); i++){ v0[i][0] = v0[i][0]/normalizar;}

    Matriz<double> v1(v0.filas(),1);
    int i = 0;
    for(;i<3000 ;i++)
    {

        v1= A* v0;

        lambda= normaInf(v1)/normaInf(v0);

        normalizar = norma2(v1);

        for(int j = 0; j < v1.filas(); j++){ v1[j][0] = v1[j][0]/normalizar; }
        if(haypocadif(v0, v1)){break;}
        v0=v1;
    }

    return lambda;
}

vector<double> metodoPotencias(Matriz<double>& A, unsigned int alpha, Matriz<double>& P, 
Matriz<double>& v){

    vector<double> autovalores;

    for(int i=0;i<alpha;i++)
    {
        double autovalori= ObtenerAutovalorMayor(A,v);

        autovalores.push_back(autovalori);

        for(int j=0;j<A.columnas();j++){ P[j][i]=v[j][0]; }

        Matriz<double> vt(1, v.filas());

        vt= v.traspuesta();

        Matriz<double> prod(v.filas(),v.filas());
        prod= v*vt*autovalori;

        A =A - prod;
    }

    return autovalores;
}





\end{verbatim}