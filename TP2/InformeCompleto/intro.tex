\section{Introducción Teórica}
%Contendrá una breve explicación de la base teórica que fundamenta los métodos involu- crados en el
%trabajo, junto con los métodos mismos. No deben incluirse demostraciones de propiedades ni
%teoremas, ejemplos innecesarios, ni definiciones elementales (como por ejemplo la de matriz
%simétrica). En vez de definiciones básicas es conveniente citar ejemplos de bibliografía adecuada.
%Una cita vale más que mil palabras.
%
\subsection{k Nearest Neighbor y Principal Component Analysis}
En el presente trabajo utilizaremos los métodos de kNN y PCA para llevar a cabo el reconocimiento de dígitos manuscritos. Comenzaremos explicando resumidamente los métodos utilizados, de una manera más general. Dado un vector $v$ y un conjunto de vectores $U$, queremos saber a que clase pertenece $v$. Nosotros sabemos a que clase pertenece cada vector $u_i \in U$, entonces una manera sencilla de "estimar" a que clase pertenece $v$ en base a la información que poseemos, es comparándolo con cada vector de $U$. Una manera de efectuar esta comparación es tomando la distancia entre $v$ y cada vector $u_i \in U$ y quedarnos con la clase del $u_min$ que minimice esa distancia sobre todos los demás. Lo que hace el algoritmo kNN es generalizar un poco esta idea. En lugar de quedarnos con la clase del $u_min$ que minimice la distancia a $v$, kNN toma las clases de los $k$ vectores $u_1, u_2, ..., u_k$ cuyas distancias sean mínimas y elege entre ellas la clase que más aparezca. Para conocer el parámetro $k$ es necesario realizar experimentos probando distintos valores y tomar una decisión en base a la calidad de los resultados.
\par El algoritmo kNN es conceptualmente fácil de entender e implementar, lo cual constituye una ventaja, pero tiene la desventaja principal que tiene es el tiempo de cómputo que requiere. Esto se debe a que, como los vectores representan imágenes, la dimensión de estos vectores suele ser grande. Dado que la idea detras del reconocimiento de imágenes en este trabajo radica en utilizar una base de datos relativamente grande (de ahora en más dbTrain) de información para determinar que tipo de imagen es la que estamos tratando de reconocer, el costo de computar estas distancias se multiplica por la cantidad de imágenes que tenemos en nuestra base de datos. Vemos que este método, así como está planteado, no escala.
\par Como ya dijimos, por cuestiones de performance no es conveniente utilizar kNN de manera directa con los datos que tenemos. Lo que vamos a intentar hacer entonces, es realizar una transformación de nuestros datos de manera tal que luego, cuando queramos aplicar kNN no nos resulte tan costoso. Recordemos que nuestros datos son vectores en un espacio ${\rm I\!R}^n$, entonces lo que vamos a querer hacer es transformarlos a vectores en otro espacio ${\rm I\!R}^\alpha$ tal que $\alpha < n$. Pero también vamos a querer que esta transformación no "cambie por completo" a nuestros vectores, en el sentido de que sigan representando las imágenes que representaban o al menos las "partes relevantes" de ellas. Para poder llevar a cabo esta transformación realizaremos los siguientes pasos:

\begin{enumerate}
\item Tomar los vectores de dbTrain en forma matricial (cada vector una fila de la matriz) y hallar la matriz de covarianza $M \in {\rm I\!R}^{nxn}$.
\item Hallar una matriz $P \in {\rm I\!R}^{nxn}$ que nos permita disminuir la covarianza de los datos de dbTrain.
\item Sea $P'$ la matriz $P$ con las primeras $\alpha$ columnas (este parámetro se fija experimentación mediante), es decir $P' \in {\rm I\!R}^{nx\alpha}$, realizamos el producto $x'_i = P'^tx_i$ donde $x_i$ es la i-ésima imagen de dbTrain y $x_i'$ es esa misma imagen luego de aplicar nuestra transformación. Ahora $x_i' \in {\rm I\!R}^{\alpha}$.
\end{enumerate}

Para obtener $P$ lo que hacemos es obtener la base ortonormal de autovectores de $M$. Sabemos que dicha base existe por ser $M$ simétrica. Entonces la columna $i$ de $P$ va a ser el vector $v_i$, el i-ésimo autovector de $M$. Esta base lo que nos permite hacer es "observar" a nuestros datos desde otro lugar, o sea ahora nuestros ejes de coordenadas van a estar en las direcciones donde más varianza existe entre los datos.

Una vez completados estos tres pasos, cuando queramos reconocer a que clase pertenece un vector $v$, trabajamos con $v'= P'^tv$ y, dado que ahora $v'$ es un vector de $\in {\rm I\!R}^{\alpha}$ al igual que los vectores de dbTrain podemos aplicar $kNN$.