\section{Desarrollo}
%Deben explicarse los métodos numéricos que utilizaron y su aplicación al problema
%concreto involucrado en el trabajo práctico. Se deben mencionar los pasos que si-
%guieron para implementar los algoritmos, las dificultades que fueron encontrando y la
%descripción de cómo las fueron resolviendo. Explicar también cómo fueron planteadas
%y realizadas las mediciones experimentales. Los ensayos fallidos, hipótesis y conjeturas
%equivocadas, experimentos y métodos malogrados deben figurar en esta sección, con
%una breve explicación de los motivos de estas fallas (en caso de ser conocidas).
\subsection{k-Nearest Neighbors (kNN):}
El primer método utilizado es kNN. Como se dijo en la introducción teórica, este método es muy intuitivo, se basa en tomar a cada imagen como un punto en el espacio, y se hace una votación de la moda de los k vecinos cercanos. Todas las imagenes de dbTrain (28x28 pixeles), se encuentran guardadas vectorizadas en una matriz de ${\rm I\!R}^{42000x785}$. La primera columna establece el label de cada imagen, las columnas restantes son los pixeles.\\
Mediante el K que determine la partición, utilizando la técnica de K-Fold Cross Validation, vamos a dividir esa matriz, en dos, una de train, con $(42000/K)*(K-1)$ imágenes y otra de test, con $42000/K$ imágenes. A su vez, vamos a eliminar la primera columna de cada matriz, y nos las vamos a guardar en 2 vectores diferentes. Asi podremos comparar cada imagen de la matriz de test, con todas las imágenes de la matriz de train y al mismo tiempo tener guardados los labels correspondientes a cada imagen. Finalmente la idea algoritmica se detalla en el siguiente pseudocódigo:\\
\begin{algorithm}[H]
\caption{kNN(int $k$, matriz Test, matriz Train, vector digTest, vector digTrain)}
\begin{algorithmic}[1]
\State reconocidos = 0
\For{z = 0; z <Test.filas() ; z++}
	\State vector<pair<int, int> > normas2AlCuadrado
	\State \textit{$\setminus\setminus$ La primera componente de cada tupla es el label del digito de la base de train(de ahora en más d), y la segunda componente es la distancia entre el digito de la base de test y d}	
	\For{m = 0; m < Train.filas(); m++}
		\State \textit{$\setminus\setminus$ calculamos las distancias}
		\State distanciaAlCuadrado = 0
		\For{i = 0; i <Test.columnas() ; i++}
			\State distanciaAlCuadrado += (Test[z][i] - Train[m][i]) *(Test[z][i] - Train[m][i])
		\EndFor
		\If{normas2AlCuadrado.size() < k} \State \textit{$\setminus\setminus$ colocamos las primeras k normas}
			\State pair<unsigned int, int> a
            \State a.first =  digTrain[m]
            \State a.second = distanciaAlCuadrado
            \State normas2AlCuadrado.pushback(a)
         \Else
         	\If{haymayor(normas2AlCuadrado, distanciaAlCuadrado}
         			\State \textit{$\setminus\setminus$  si ya tengo k voy sacando las mayores distancias}
         		    \State int posmayor = dondemayor(normas2AlCuadrado)
                    \State normas2AlCuadrado[posmayor].first = digTrain[m]
                    \State normas2AlCuadrado[posmayor].second = distanciaAlCuadrado
             \EndIf
         \EndIf

	\EndFor
	
\EndFor
\State \textit{$\setminus\setminus$  Ahora hago la votación entre los k vecinos}
\State digitos[10] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }
\For{int t = 0; t < k; t++}
	\State digitos[normas2AlCuadrado[t].first]++
\EndFor
\State ganador = 0
\For {int x = 0; x < 10; x++}
	\If {digitos[x] > digitos[ganador]}
				\State  ganador = x
			\EndIf
\EndFor
\State \textit{$\setminus\setminus$  Si en la votación gano el verdadero label de la imagen de test, sumamos reconocidos}
\If {ganador == digTest[z]}
				\State  reconocidos++
\EndIf

\State tasaDeReconocimiento = reconocidos/Test.filas()


\end{algorithmic}
\end{algorithm}

\subsection{Principal Component Analysis (PCA):}


El segundo método utilizado es PCA. Dado que las imagenes estan representadas por vectores y las dimensiones de los mismos pueden ser muy grandes, a la hora de analizar las imagenes se requiere una gran cantidad de tiempo de computo así como de memoria. PCA tiene como objetivo redimensionar dichos vectores, siempre que se obtenga un determinado compromiso entre las nueva cantidad de variables y la calidad de las imagenes que ahora se representan. Para esto se construye un nuevo sistema de ecuaciones donde en el eje i se representa la i-esima varianza de mayor valor para el conjunto original de imagenes dadas. De esta manera cada variable esta correlacionada, siendo las primeras las de mayor importancia, por lo que se pueden obviar las últimas componentes (ya que serían las que menos importancia tienen) reduciendo el número de variables. Para poder llevar a cabo esta transformacion lineal es necesario construir la matriz de covarianza para el conjunto de valores que representan a las imagenes.
Para esto, sean n observaciones (imagenes) y dadas dos coordenadas $x_i$, $x_k$ su covarianza se obtiene como: \newline


$\sigma_{x_j x_k}$ = $(1/(n-1))\sum_{i=1}^{n}(x_{j}^{(i)}-\mu_j)(x_{k}^{(i)}-\mu_k)$  = $(1/(n-1))(x_{k}^{(i)}-\mu_kv)^{t}(x_{j}^{(i)}-\mu_jv)$ \newline

\textit{con $\mu_j$, $\mu_k$ la media de las coordenadas j y k respectivamente; $x_{j}^{(i)}$, $x_{k}^{(i)}$ las coordenadas j y k de la i-esima imagen vectorizada y $v^{t}$ = (1,.....,1).}\newline

De esta forma la covarianza entre dos muestras puede ser calculado como el producto entre dos vectores. Por lo que si A $\epsilon$ $\Re^{nx784}$ representa n imagenes de train vectorizadas de 784 variables cada una, la covarianza pueden ser calculadas como:\newline

 $M_x$ = $A'^{t}*A'$; \ \ \ \ \textit {con $A'=A/\sqrt{n-1}$ y $M_x$ $\epsilon$  $\Re^{784x784}$} \newline

obteniendo en la diagonal la varianza de cada coordenada. Con el fin de disminuir las redundancias (covarianza), se plantea un cambio de base. Para esto se diagonaliza la matriz $M_x$, de forma tal que: \newline

 $M_x$ = $P*D*P^{t}$; \ \ \ \ \ \textit{D diagonal y P matriz ortogonal con los autovectores de A como columnas$^{1}$} \newline%TEOREMA 9.10 I COROLARIO 9.11 BURDEN PAG 555.
 
 Para obtener los autovectores de $M_x$ se empleó el método de la potencia$^{2}$ %PAG 560 BURDEN
. Dado que la misma es una técnica iterativa que converge al autovector asociado, se estableció que la misma se detenga luego de alcanzar un maximo de 3000 iteraciones o cuando la diferencia entre cada coordenada del vector obtenido en la iteración k respecto de la k+1 sea muy poca (en este caso optamos por 0.0000009). Una vez obtenido el primer autovector procedimos a calcular el segundo, aplicando el método de deflación$^{3}$ %pag 570 teorema 9.15 burden.
.Ambas técnicas serán aplicadas un número $\alpha$ de veces. En la sección experimentación se evaluará como se comportan los resultados y el tiempo al variar este valor.
Una vez obtenidos los $\alpha$ autovectores se procedió a llevar a cabo el cambio de base, tanto para las imagenes utilizadas como train como para las de test, por lo que se multiplicó cada autovector obtenido por las imagenes vectorizadas en A': \newline

$v_i^{t}*a^{(i)}$ ; \textit{$v_i$ el i-esimo autovector y $a^{(i)}$ la i-esima imagen vectorizada} \newline

lo que matricialmente se puede obtener al hacer: A'*P (llamaremos a la matriz resultante $TcTrain$). De esta forma la matriz TcTrain $\epsilon$ $\Re^{n*\alpha}$. Para poder comparar las imagenes de test (matriz B) con TcTrain, es necesario llevar a las imagenes de test a las mismas dimensiones, por lo que a cada una se le restara la media que corresponda y dividirá por $\sqrt{n-1}$, obteniendo $ B'$. 
Una vez realizado esto procedemos a cambiar la base tal como se hizo para las de train:\newline
 TcTest = $B'*P$. En las matrices TcTrain y TcTest se encuentran las nuevas coordenadas de las imágenes(con reducción de la dimensión) almacenadas como filas.
 

 Finalizado los cambios de bases obtenemos imagenes en un mismo espacio vectorial cuyas dimesiones son menores a las originales. Pudiendo aplicar distintas técnicas para llevar a cabo el reconocimiento de los dígitos. Optaremos por aplicar KNN para así poder comparar las diferencias entre tiempo de computo y análisis cuando se busca reconocer dígitos utilizando PCA + KNN y solo KNN.
 
A continuación se muestra un pseudocódigo de los procedimientos descriptos anteriormente:

\begin{algorithm}[H]
\caption{PCA(matriz Train, matriz Test, int $\alpha$)}
\begin{algorithmic}[1]
\State n = cantidad de imagenes de train
\State \textit{$\setminus\setminus$Se procede a calcular el promedio de las imagenes de train}
\State $ vector Promedio \gets crear(784)$
\For{i = 0; i < 784; i++}
	\State sum = 0	
	\For{j = 0; i < n; j++}
		\State sum = sum + Train[j][i]
	\EndFor
	\State promedio[i] = sum/n
\EndFor

\State \textit{$\setminus\setminus$ calculamos la matriz de covarianza:}
\For {i = 0; i < n; i++}
	\For {j = 0; j < 784; j++}
		\State Train[i][j] = Train[i][j] - promedio[j]
	\EndFor
	\State Train[i][j] = Train[i][j]/$\sqrt{n-1}$
\EndFor	

\State $matriz$ $covarianza$ $\gets$ $Train^{t}*Train$	
\State \textit{$\setminus\setminus$ obtenemos la matriz con los $\alpha$ autovectores de la matriz de covarianza como columna (P):}
\State $P$ $\gets$ metodo de la potencia y de deflacion (covarianza, $\alpha$)

\For {i = 0; i < cantidad de imagenes de test; i++}
	\For {j = 0; j < 784; j++}
		\State test[i][j] = test[i][j] - promedio[j]
	\EndFor
	\State test[i][j] = test[i][j]/$\sqrt{n-1}$
\EndFor	

\State \textit{$\setminus\setminus$ realizamos la transformaci\'on caracteristica para train (TcTrain) y para test (TcTest)):}
\State  TcTrain $\gets$ $Train*P$
\State  TcTest $\gets$ $Test*P$


\State \textit{$\setminus\setminus$ finalmente procedemos a reconocer los d\'igitos:}
\State Knn(tcTriain, TcTest)
\end{algorithmic}
\end{algorithm}

\subsection{K-Fold Cross Validation}
Para poder concentrarnos en la evalución de los métodos y la óptima eleccion de los parámetros, necesitamos probar los mismos sobre la base de train, ya que sobre sus elementos conocemos el digito al que pertenece cada uno. Ante esta situación, particionaremos dicha base en dos, utilizando una parte de ella en forma completa para el training y la restante como test, pudiendo asi verificar la predicción realizada y mostrar las tasas de efectividad (La cantidad de dígitos correctamente clasificados respecto a la cantidad total de casos analizados).\\
Sin embargo, surge un problema no menor, realizar la experimentación sobre una única partición podría traer como consecuencia predicciones no deseadas, y una mala estimación de los parámetros, por ejemplo podría ocacionar $overfittin$, que el algoritmo del método bajo análisis quede ajustado a una serie de características muy específicas de la base de train que no tienen relación con el obejeto de estudio. Para soluconar el problema en cuestión, se usará la técnica de Cross Validation, en particular el K-Fold Cross Validation, para que los resultados de la experimentación resulte estadísticamente más robustos.\\
La técnica de K-Fold Cross Validation consiste en particionar de forma aleatoria la base de train en K conjuntos de igual tamaño (Siempre asumiendo que el cardinal del conjunto de la base de train es múltiplo de K).
Luego se realizan K ejecuciones del programa, cada una de ellas eligiendo un conjunto para test, y utilizando los K-1 conjuntos restantes para train. Para las K ejecuciones se usarán los mismos parámetros en cada una de ellas. Además, en el método tradicional se suelen realizar varias corridas para un mismo valor de K.\\
Para calcular estas particiones usaremos el comando CVPARTITION de MATLAB de la siguiente manera:\\
\begin{algorithm}[H]
\caption{calcularParticion(int $K$, int $n$)}
\begin{algorithmic}[1]
\State C = cvpartition(n,'KFold',K)
\State fileID = fopen('K.txt','w');
\For{int i = 1:K}
	\State fprintf(fileID,'d ',C.training(i)')
	\State fprintf('$n$')
\EndFor

\State fclose(fileID)

\end{algorithmic}
\end{algorithm}
\subsection{Experimento 1: Variando el k en kNN}
Se llevará a cabo una cierta cantidad de experimentaciones
para poder contrastar la siguiente hipótesis:

\begin{itemize}
\item Hipótesis N$ ^{\circ} $ 1: En el método de kNN, al incrementar k vamos obteniendo una mayor tasa de efectividad. En otras palabras, se resumiría que a mayor valor de k, mayor porcentaje de la tasa.
\end{itemize}
%y variando los k, con k $\in$ $\{$2, 30, 100, 500$\}$.

Para verificar la hipótesis N$ ^{\circ} $ 1, se llevará a cabo una experimentación corriendo el programa con la base de train de Kaggle, utilizando la técnica de K-Fold cross validation, con 2, 10 y 20 particiones.
 En este caso, como se ejecutará solo el método de kNN sin ningún complemento, la elección del parametro $\alpha$ no incide en la misma.\\ Luego se procederá a realizar un análisis de las tasas de efectividad resultantes.

\subsection{Experimento 2: variando el $\alpha$ en kNN + PCA}

Ante lo observado y desarrollado, se llevará a cabo una cierta cantidad de experimentaciones
para poder contrastar la siguiente hipótesis:

\begin{itemize}
\item Hipótesis N 2: En el método de kNN aplicandole el complemento de PCA, manteniendo K constante, diferencias notables de $\alpha$ reflejan diferencias notables en las tasas de efectividad. Más profundamente, a mayor valor de $\alpha$ mayor nivel de la tasa y a menor valor de $\alpha$ menor nivel de la tasa
\end{itemize}

Para verificar la hipótesis N 2, se llevará a cabo una experimentación corriendo el programa con la base de train de Kaggle, tomando como parámetros K = 2, 10 y 20, k = 2 y variando los $\alpha$, con $\alpha$ $\in$ $\{$20, 50, 100, 300, 500$\}$. La eleccion del k, se baso en las experimentaciones anteriores, dicho valor escogido fue el de mejor resultado en cuanto a tiempo y tasa de efectividad\\ Luego se procederá a realizar un análisis de las tasas de efectividad resultantes.

\subsection{Experimento 3: variando el K}
Ante lo observado y desarrollado, se llevará a cabo una cierta cantidad de experimentaciones
para poder contrastar las siguientes hipótesis:

\begin{itemize}
\item Hipótesis N 3: En cualquiera de los 2 métodos implementados fijando k y $\alpha$, a mayor valor de K, mayor porcentaje de tasas.
\item Hipótesis N 4: El tiempo de cómputo de kNN para distintos valores de K y un valor fijo de k no debería ser muy distinto, en cambio para kNN+PCA, fijando k y $\alpha$, a mayor valor de K, mayor tiempo de computo.
\end{itemize}

Para verificar la hipótesis N 3, se llevará a cabo una experimentación corriendo el programa con la base de train de Kaggle....\\
Para verificar la hipótesis N 3, se llevará a cabo una experimentación corriendo el programa con la base de train de Kaggle....
 \\ Luego se procederá a realizar un análisis de las tasas de efectividad resultantes.


