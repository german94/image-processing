\section{Conclusiones}
%Esta sección debe contener las conclusiones generales del trabajo. Se deben mencionar
%las relaciones de la discusión sobre las que se tiene certeza, junto con comentarios
%y observaciones generales aplicables a todo el proceso. Mencionar también posibles
%extensiones a los métodos, experimentos que hayan quedado pendientes, etc.

En el presente trabajo pudimos apreciar que la utilización de los métodos KNN y PCA, para ciertas configuraciones (de $\alpha$ y $k$), nos proveen tasas de reconocimiento bastante altas. Dichas configuraciones requieren tiempos de cómputo muy razonables ya que vimos que aumentar demasiado $\alpha$ no tiene sentido y que a valores muy grandes de $k$ la tasa disminuye. Es decir, obtener tasas de reconocimiento altas implica utilizar parámetros de $k$ y $\alpha$ relativamente chicos y como el tiempo de cómputo se ve afectado negativamente cuando estos parámetros son grandes, entonces los tiempos asociados a tasas altas no son necesariamente grandes.

Sin embargo, el método de KNN tiene un margen de error a la hora de reconocer dígitos. Este error se debe en parte a la cercanía de dígitos distintos pero también a los errores numéricos que se propagan al hacer operaciones aritméticas en la computadora como por ejemplo calcular la norma de la resta de cada vector.

PCA es un método que resulta muy efectivo para el reconocimiento de dígitos por varios motivos. En teoría deberían obtenerse tasas menores que con KNN, ya que se descarta información cuando se reduce la dimensión. Sin embargo, PCA arrastra menos error numérico y en consecuencia en nuestros experimentos obtenemos tasas mayores. Hipotéticamente, tambien pensamos que esta situacion se deba a que PCA, aun asi siendo el $\alpha$ no tan grande, los datos que capture sean la mayoria de los datos esenciales de cada imagen, descartando los datos que traerían ruido al procesamiento con kNN. En un futuro, nos gustaría experimentar más a fondo, para determinar cual de estas dos situaciones es realmente la que pasa, o cual es el grado de cupabilidad de cada una. \\
Por otro lado, este método al tratar con menos cantidades de coordenadas para las imágenes el tiempo de cómputo es mucho menor.

Utilizando cross validation pudimos realizar experimentos cuyos resultados nos proveen información objetiva, ya que las particiones son generadas de manera aleatoria y la tasa final de un método para cierta configuración es el promedio de las tasas de cada partición. En algunos casos, al trabajar con valores de $K$ muy grandes tuvimos que realizar aproximaciones sobre el tiempo de cómputo total que tardaría la computadora en procesar todas estas particiones y utilizar la tasa de reconocimiento de una partición sola. De todas formas, esta información si bien está aproximada, nos permite ver como se puede agrandar la brecha entre el método de KNN y PCA + KNN para valores de $K$ mayores.

Sería interesante realizar, en un futuro, experimentos que permitan ver para que configuraciones KNN es mejor que PCA + KNN en la práctica ya que en nuestros resultados siempre es mejor PCA + KNN, tanto en tiempo de cómputo como en tasas de reconocimiento. Para esto, deberíamos trabajar con una cantidad de valores de $\alpha$ y $k$ más grande. También queda pendiente considerar otras normas para las distancias entre las imágenes, y ver que tan efectivo es el método. Además, en el marco de integración de todas las materias de la carrera, nos gustaria probar si se puede acelerar el computo de operacion entre vectores o matrices, usando lo que aprendimos en Organización del Computador II sobre procesamiento paralelo usando SIMD en procesadores con arquitectura Intel x86.

