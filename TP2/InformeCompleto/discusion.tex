\section{Discusión}
%Se incluirá aquí un análisis de los resultados obtenidos en la sección anterior (se analizará
%su validez, coherencia, etc.). Deben analizarse como míınimo los ítems pedidos en el
%enunciado. No es aceptable decir que “los resultados fueron los esperados”, sin hacer
%clara referencia a la teoría la cual se ajustan. Además, se deben mencionar los resul-
%tados interesantes y los casos “patológicos” encontrados.





\subsection{Hipótesis 1:}
Con respecto a la hipótesis 1, que establecía que corriendo cualquiera de los 2 métodos implementados(kNN y kNN+PCA) fijando k y $\alpha$, a mayor valor de K mayor porcentaje de tasas, procederemos primero a observar los resultados de kNN+PCA.\\ Comparando las tasas de efectivad resultantes de usar K=2 contra K=10 y K=20, se nota claramente que las de K=2 son menores que las de los otros dos valores de K. Esto es consecuencia principal del uso de la técnica de K-Fold Cross Validation, ya que con un valor K chico nuestra partición de la base de entrenamiento se divide en partes mas grandes, en consecuencia la longitud de la base de test tiende a ser igual a la base de entrenamiento. Dicho en otros términos, tenemos menos elementos en la base de entrenamiento para entrenar nuestro algoritmo y menos permutaciones sobre cuales elementos los consideramos parte del test. Esto es la principal causa de que con K=2 obtengamos los menores valores de la tasa de efectivida. Para que la tasa de efectividad tienda a 1, es necesario contar con una base lo más extensa posible. Y además al aumentar K, tenemos mas combinacion de particiones posibles y un entrenamiento más profundo. En este caso, la disminucion de la misma no se ve afectada por el valor que tome k y alpha. \\Hasta aqui la hipótesis se confirmaría, pero al analizar y comparar las tasas de K=10 contra K=20, observamos que aca si incide el valor que tome k. Para k con valores chicos, se ve una diferencia de la tasa cuando saltamos de K=10 a K=20, pero a medida que k aumenta, las tasas empiezan a comportarse de manera similar, teniendo vaivenes para algunos k, en el que la tasa de K=10 le gana a K=20 o viceversa. Ante esta situación, decidimos observar más detalladamente los datos de la experimentación y logramos ver que para k grandes y K=20, la muestra de las tasas de cada partición posee una mayor varianza, es decir que los valores tienen mayores fluctuaciones en comparación a la media de la muestra. Realizando un análisis más intensivo, deducimos que esto se debe a que a mayor valor de K, tengo mas elementos de entrenamiento, y a su vez a mayor valor de k, tengo más elemento que entrarán en la definición de vecinos más cercanos, pudiendo aqui encontrar elementos que no son del mismo label que el elemento en estudio y así alterar notablemente la tasa de efectividad en cada una de las particiones.\\
Como último dato, el valor de $\alpha$ no incide en las tasas de efectividad entre diferentes K, es decir, para un $\alpha$ fijo, a medida que se aumenta el valor de K, aumenta la tasa.\\
Con respecto al método de kNN, la elección de los K, se comporta de la misma manera y tiene las mismas consecuencias que en el método de kNN+PCA.
Finalmente se pordría concluir que la hipótesis queda parcialmente verificada, ya que la única parte que se refuto de la misma fue la parte de mantener k constante. Sobre esta particularidad de k, se continuará detallando en la discución del experimento 1.

\textbf{Análisis de tiempo de computo al variar K:}
 En el método de kNN+PCA, claramente se incrementa el tiempo de cómputo a medida que aumenta el valor de K, esto se debe a que el cálculo de la matriz de covarianza del entrenamiento domina gran parte del tiempo del metodo (sin contar la llamada a kNN). Dicha matriz se calcula de la siguiente forma: por ejemplo, si K=2, la matriz de entranamiento va a tener un tamaño de 21000x784, su transpuesta de 784x21000, entonces la matriz de covarianza se calcula multiplicando la transpuesta por la matriz de entrenamiento resultando siempre una matriz de 784x784. Para este caso de K, haremos 2 multiplicaciones de matrices de 784x21000 y 21000x784. Ahora bien, si suponemos que K=10, el procedimiento es el mismo, salvo que ahora, mi base de entrenamiento va a ser mas grande, por ende, su matriz también que en este caso va a tener tamaño de (42.000-42.000/10)x784 = 37800x784. Para este caso de K, haremos 10 multiplicaciones de matrices de 784x37.800 y 37.800x784. Al aumentar el valor de K, además de hacer más multiplicaciones por tener más particiones, las matrices aumentan de tamaño por lo explicado anteriormente. Para K=20 es el mismo procedimiento.\\
En el método de kNN, por ejemplo para K=2, voy a recorrer 2 veces la base de entrenamiento de 21000 dígitos para un base de test con la misma cantidad de dígitos. Para k=10, voy a recorrer 10 veces la base de train de 37.800 dígitos para una base de test de 4.200 digitos. Realizando un análisis más exhaustivo, para el primer caso de K=2, estariamos recorriendo 21.000 veces la base de 21.000 digitos, todo eso 2 veces lo que resultaria en 2*21.000*21.000 = 822.000.000 iteraciones. Para el caso de K=10, estariamos recorriendo 4200 veces la base de 37800 digitos, todo eso 10 veces lo que resultaria en 10*4.200*37.800 = 1.587.600.000 iteraciones, lo que implacarŕia un 93$\%$ más de iteraciones. Para K=20 pasa lo mismo.\\

 \subsection{Experimento 1:}

Podemos ver que en ningún caso se obtiene una tasa del 100$ \% $. Con un k chico se corre el riesgo de que los vecinos más cercanos no sean los correctos por una situación especial que pueda llegar a ocurrir, y entonces el dígito no sea reconocido, por ejemplo que justo el dígito en estudio este rodeado de k dígitos con otra caracterización. Se verifica empíricamente la hipótesis N$ ^{\circ} $ 2, con un k grande las tasas disminuyen apreciablemente. Esto se lo podemos atribuir a que hay una probabilidad no despreciable de que un dígito a reconocer este mas cercano a un dígito con distinta caracterizacón que a uno con igual. No es una cuestión de falta de dígitos iguales al que se esta reconociendo. Por ejemplo al tomar 500 vecinos mśs cercanos, no es que no halla sido posible tomar a estos 500 como el dígito a reconocer por falta de este ultimo en la matriz de train, sino que en lugar de tomarse un dígito con igual caracterización, se toma uno con distinta, pero más cercano.

También podemos ver que al incrementar k se obtiene un tiempo mayor. Esto se debe a que el algoritmo de knn en cada paso, recorre un vector de tamaño k donde se encuentran los mas cercanos, y lo va ir actualizando. 



 \subsection{Experimento 2:}
\subsubsection{Variando k}
Al realizar las variaciones de $k$ propuestas en el desarrollo obtuvimos distintas tasas de reconocimiento que, como podemos observar en la primera tabla, van empeorando a medida que aumenta el $k$. Esto pasa porque, aunque estemos haciendo PCA primero, luego vamos a estar aplicando KNN y ya vimos que cuando aplicamos KNN y aumentamos el valor de $K$, las tasas empeoran, independientemente del preprocesamiento de los datos.
\subsubsection{Variando $\alpha$}
En los experimentos vimos que nuestra hipótesis de que al aumentar el valor de $\alpha$ ibamos a estar aumentando la tasa de reconocimiento es falsa. Sin embargo, cuando bajamos de $\alpha = 50$ a $\alpha = 5$ pudimos notar una diferencia importante en la tasa de reconocimiento. Una posible explicación a lo que está sucediendo es que, si bien matemáticamente tiene sentido pensar que deberíamos tener tasas de reconocimiento más altas al aumentar $\alpha$ (porque agrandamos nuestra base en la cual expresamos nuestros datos), nosotros estamos implementando todo esto en una computadora. Esto significa que todo lo que hacemos tiene cierto error y que ese error puede ir incrementandose. 
\par Los autovectores por ejemplo, los calculamos mediante el método de la potencia, pero no debemos olvidarnos que el método de la potencia se basa en aproximar un límite. Esto quiere decir que los autovalores y autovectores que estamos calculando, en realidad no van a ser exactamente los que queremos obtener. En la experimentación vimos que a partir de un cierto valor, ya no tiene sentido seguir aumentando $\alpha$ porque las tasas dan muy similares (incluso peores) y el tiempo de cómputo aumenta. Tranquilamente lo que podría estar pasando es que, los primeros autovectores que estemos calculando tengan menos error (porque el método de la potencia para ellos converge más rapidamente) y a medida que se va aumentando la cantidad de autovectores, el error que se produce al calcular cada uno de ellos es cada vez más grande. Esto explicaría lo que pasa en nuestros resultados porque los primeros autovectores de la base son los que estarían aportando más información (porque son los que menos error tendrían) y a partir de un punto, el error es tan grande que ya no aporta información confiable y puede ocurrir entonces que la tasa de reconocimiento empeore.



\subsection{Competencia Kaggle}
Para realizar nuestro submission en la página de Kaggle, a la competencia de Digit Recognizer, modificamos levemente el codigo de nuestro programa. El mismo se encuentra en la carpeta Kaggle. Las modificaciones se deben a que en este caso, la base de test proporcionada por la página no tiene labels, no utilizamos el método de K-Fold Cross Validation, la funcion kNN ahora tiene que devolver las predicciones de los dígitos, entre otras cosas.\\
Luego de toda la experimentación concluímos que los mejores parámetros, en cuanto tiempo y eficacia, para correr el programa son algún k y $\alpha$ cercanos a:
\begin{itemize}
\item $\alpha$ = 50
\item $k$ = 2
\end{itemize}

A continuación seguiremos experimentando y probaremos con valores cercanos a estos parámetros para estudiar la efectividad que nos proporciona el sistema de Kaggle.
\begin{itemize}
	\item Con k=2 y $\alpha$=50, tuvimos una efectividad de 0.96686 con un tiempo de computo de 1338.35 
	\item Con k=3 y $\alpha$=50, tuvimos una efectividad de 0.96857 con un tiempo de computo de 1454.45.
	\item Con k=3 usando solo kNN, tuvimos una efectividad de 0.97200 con un tiempo de computo de 12691.6.
	\item Con k=4 y $\alpha$=50, tuvimos una efectividad de 0.97286 con un tiempo de computo de 1654.71.
	\item Con k=5 y $\alpha$=50, tuvimos una efectividad de 0.97286 con un tiempo de computo de 1776.28.
	\item Con k=4 y $\alpha$=60, tuvimos una efectividad de 0.97314 con un tiempo de computo de 1950.58.
	\item Con k=10 y $\alpha$=70, tuvimos una efectividad de 0.97086 con un tiempo de computo de 3050.58.
	\item Con k=1 y $\alpha$=60, tuvimos una efectividad de 0.97200 con un tiempo de computo de 1750.48

\end{itemize}